{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
    "\n",
    "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P59NYU98GCb9"
   },
   "outputs": [],
   "source": [
    "#!pip3 -qq install torch==0.4.1\n",
    "#!pip3 -qq install bokeh==0.13.0\n",
    "#!pip3 -qq install gensim==3.6.0\n",
    "#!pip3 -qq install nltk\n",
    "#!pip3 -qq install scikit-learn==0.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sVtGHmA9aBM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6CNKM3b4hT1"
   },
   "source": [
    "# Рекуррентные нейронные сети (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_XkoGNQUeGm"
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFEtWrS_4rUs"
   },
   "source": [
    "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPIkKdFlHB-X"
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiA2dGmgF1rW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Mikhail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d93g_swyJA_V"
   },
   "source": [
    "Пример размеченного предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QstS4NO0L97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epdW8u_YXcAv"
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTai8Ta0lgwL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count in train set: 739769\n",
      "Words count in val set: 130954\n",
      "Words count in test set: 290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
    "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
    "print('Words count in test set:', sum(len(sent) for sent in test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eChdLNGtXyP0"
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCjwwDs6Zq9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train = 45441. Tags = {'CONJ', 'PRT', 'ADP', 'PRON', 'ADJ', '.', 'ADV', 'NOUN', 'NUM', 'VERB', 'DET', 'X'}\n"
     ]
    }
   ],
   "source": [
    "words = {word for sample in train_data for word, tag in sample}\n",
    "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for word, tag in sample}\n",
    "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URC1B2nvPGFt"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gArQwbzWWkgi"
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rWmSToIaeAo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unigram tagger = 92.62%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Ymb_MkbWsF"
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjz_Rk0bbMyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger = 93.42%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWMw6QHvbaDd"
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XCuxEBVbOY_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger = 23.33%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t3xyYd__8d-"
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:  \n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtRbz1SwgEqc"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind):\n",
    "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhsTKZalfih6"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    n_samples = len(X)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start:end]\n",
    "        \n",
    "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4XsRII5kW5x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 4), (32, 4))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5I9E9P6eFYv"
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVEHju54d68T"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=word_emb_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=word_emb_dim, hidden_size=lstm_hidden_dim, num_layers=lstm_layers_count, proj_size=tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_HA8zyheYGH"
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08695652173913043\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)\n",
    "\n",
    "def compute_accuracy(predictions, targets):\n",
    "    _, indices = torch.max(predictions, 2)\n",
    "    mask = targets > 0\n",
    "    correct_count = torch.sum(indices[mask] == targets[mask]).item()\n",
    "    total_count = torch.sum(mask).item()\n",
    "    return float(correct_count), float(total_count)\n",
    "\n",
    "correct_count, total_count = compute_accuracy(logits, y_batch)\n",
    "print(correct_count / total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMUyUm1hgpe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(81.8755, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def compute_loss(predictions, targets, criterion=nn.CrossEntropyLoss(ignore_index=0)):\n",
    "    seq_size = len(predictions)\n",
    "    loss = 0\n",
    "    for i in range(seq_size):\n",
    "        loss += criterion(predictions[i], targets[i])\n",
    "    return loss\n",
    "\n",
    "print(compute_loss(logits, y_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSgV3NPUpcjH"
   },
   "source": [
    "**Задание** Вставьте эти вычисления в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FprPQ0gllo7b"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "                logits = model(X_batch)\n",
    "\n",
    "                loss = compute_loss(logits, y_batch, criterion=criterion)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                cur_correct_count, cur_sum_count = compute_accuracy(logits, y_batch)\n",
    "\n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
    "                )\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, correct_count / sum_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
    "        val_data=None, val_batch_size=None):\n",
    "        \n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pqfbeh1ltEYa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 30] Train: Loss = 50.81328, Accuracy = 73.41%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 48.17it/s]\n",
      "[1 / 30]   Val: Loss = 46.39259, Accuracy = 84.41%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 70.27it/s]\n",
      "[2 / 30] Train: Loss = 23.35741, Accuracy = 87.37%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.72it/s]\n",
      "[2 / 30]   Val: Loss = 31.13580, Accuracy = 89.67%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 71.20it/s]\n",
      "[3 / 30] Train: Loss = 16.02266, Accuracy = 91.30%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.86it/s]\n",
      "[3 / 30]   Val: Loss = 24.17080, Accuracy = 92.13%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 70.46it/s]\n",
      "[4 / 30] Train: Loss = 12.02370, Accuracy = 93.39%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.26it/s]\n",
      "[4 / 30]   Val: Loss = 21.26518, Accuracy = 93.31%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 71.62it/s]\n",
      "[5 / 30] Train: Loss = 9.12879, Accuracy = 94.64%: 100%|███████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.37it/s]\n",
      "[5 / 30]   Val: Loss = 17.63390, Accuracy = 94.07%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 71.42it/s]\n",
      "[6 / 30] Train: Loss = 7.23490, Accuracy = 95.52%: 100%|███████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.38it/s]\n",
      "[6 / 30]   Val: Loss = 19.51849, Accuracy = 94.47%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 70.84it/s]\n",
      "[7 / 30] Train: Loss = 5.78180, Accuracy = 96.16%: 100%|███████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.60it/s]\n",
      "[7 / 30]   Val: Loss = 16.53107, Accuracy = 94.86%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 71.81it/s]\n",
      "[8 / 30] Train: Loss = 4.76399, Accuracy = 96.65%: 100%|███████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.25it/s]\n",
      "[8 / 30]   Val: Loss = 16.37149, Accuracy = 95.06%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 70.08it/s]\n",
      "[9 / 30] Train: Loss = 3.99669, Accuracy = 97.07%: 100%|███████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.27it/s]\n",
      "[9 / 30]   Val: Loss = 15.31022, Accuracy = 95.21%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 72.82it/s]\n",
      "[10 / 30] Train: Loss = 3.37489, Accuracy = 97.38%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.44it/s]\n",
      "[10 / 30]   Val: Loss = 16.44043, Accuracy = 95.34%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 76.23it/s]\n",
      "[11 / 30] Train: Loss = 2.87132, Accuracy = 97.67%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.32it/s]\n",
      "[11 / 30]   Val: Loss = 18.39870, Accuracy = 95.43%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 70.08it/s]\n",
      "[12 / 30] Train: Loss = 2.44162, Accuracy = 97.92%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.56it/s]\n",
      "[12 / 30]   Val: Loss = 18.46500, Accuracy = 95.48%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 72.02it/s]\n",
      "[13 / 30] Train: Loss = 2.12405, Accuracy = 98.15%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.37it/s]\n",
      "[13 / 30]   Val: Loss = 18.68842, Accuracy = 95.46%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 67.89it/s]\n",
      "[14 / 30] Train: Loss = 1.82894, Accuracy = 98.35%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.58it/s]\n",
      "[14 / 30]   Val: Loss = 20.16037, Accuracy = 95.52%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 68.97it/s]\n",
      "[15 / 30] Train: Loss = 1.56990, Accuracy = 98.53%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.38it/s]\n",
      "[15 / 30]   Val: Loss = 19.04451, Accuracy = 95.56%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 72.02it/s]\n",
      "[16 / 30] Train: Loss = 1.33473, Accuracy = 98.72%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.46it/s]\n",
      "[16 / 30]   Val: Loss = 19.33643, Accuracy = 95.54%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 69.89it/s]\n",
      "[17 / 30] Train: Loss = 1.14801, Accuracy = 98.86%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.48it/s]\n",
      "[17 / 30]   Val: Loss = 19.57311, Accuracy = 95.52%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 72.02it/s]\n",
      "[18 / 30] Train: Loss = 0.99114, Accuracy = 99.00%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.47it/s]\n",
      "[18 / 30]   Val: Loss = 20.13537, Accuracy = 95.51%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 72.82it/s]\n",
      "[19 / 30] Train: Loss = 0.85099, Accuracy = 99.12%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.00it/s]\n",
      "[19 / 30]   Val: Loss = 21.04161, Accuracy = 95.54%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 70.84it/s]\n",
      "[20 / 30] Train: Loss = 0.73851, Accuracy = 99.24%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.29it/s]\n",
      "[20 / 30]   Val: Loss = 21.07394, Accuracy = 95.44%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 74.91it/s]\n",
      "[21 / 30] Train: Loss = 0.62237, Accuracy = 99.35%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.55it/s]\n",
      "[21 / 30]   Val: Loss = 23.15395, Accuracy = 95.40%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 70.46it/s]\n",
      "[22 / 30] Train: Loss = 0.62861, Accuracy = 99.35%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.12it/s]\n",
      "[22 / 30]   Val: Loss = 23.54059, Accuracy = 95.42%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 76.68it/s]\n",
      "[23 / 30] Train: Loss = 0.47546, Accuracy = 99.49%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 48.93it/s]\n",
      "[23 / 30]   Val: Loss = 23.79374, Accuracy = 95.40%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 59.52it/s]\n",
      "[24 / 30] Train: Loss = 0.42690, Accuracy = 99.54%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 48.52it/s]\n",
      "[24 / 30]   Val: Loss = 23.08158, Accuracy = 95.35%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 50.42it/s]\n",
      "[25 / 30] Train: Loss = 0.36593, Accuracy = 99.59%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.33it/s]\n",
      "[25 / 30]   Val: Loss = 27.81438, Accuracy = 95.41%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 72.21it/s]\n",
      "[26 / 30] Train: Loss = 0.29611, Accuracy = 99.66%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.38it/s]\n",
      "[26 / 30]   Val: Loss = 26.40613, Accuracy = 95.29%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 72.82it/s]\n",
      "[27 / 30] Train: Loss = 0.26689, Accuracy = 99.69%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.89it/s]\n",
      "[27 / 30]   Val: Loss = 27.67423, Accuracy = 95.29%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 74.91it/s]\n",
      "[28 / 30] Train: Loss = 0.24221, Accuracy = 99.72%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.78it/s]\n",
      "[28 / 30]   Val: Loss = 31.25983, Accuracy = 95.29%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 70.84it/s]\n",
      "[29 / 30] Train: Loss = 0.22480, Accuracy = 99.73%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 49.54it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[29 / 30]   Val: Loss = 27.86406, Accuracy = 95.22%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 74.06it/s]\n",
      "[30 / 30] Train: Loss = 0.33847, Accuracy = 99.64%: 100%|██████████████████████████████████████████████████| 572/572 [00:11<00:00, 50.03it/s]\n",
      "[30 / 30]   Val: Loss = 29.13842, Accuracy = 95.11%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 74.06it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=30,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0qGetIhfUE5"
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAfV2dEOfHo5"
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98wr38_rw55D"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: Loss = 17.71906, Accuracy = 95.13%: 100%|███████████████████████████████████████████████████████████| 224/224 [00:01<00:00, 143.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9513235491567086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = do_epoch(model, criterion, (X_test, y_test), 64, None, 'Test:')\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUTSFaEHbDG"
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 30] Train: Loss = 61.16559, Accuracy = 70.52%: 100%|██████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.96it/s]\n",
      "[1 / 30]   Val: Loss = 51.06379, Accuracy = 83.56%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 64.53it/s]\n",
      "[2 / 30] Train: Loss = 25.37021, Accuracy = 86.59%: 100%|██████████████████████████████████████████████████| 572/572 [00:12<00:00, 44.22it/s]\n",
      "[2 / 30]   Val: Loss = 30.99640, Accuracy = 89.01%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 72.39it/s]\n",
      "[3 / 30] Train: Loss = 17.54389, Accuracy = 90.75%: 100%|██████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.72it/s]\n",
      "[3 / 30]   Val: Loss = 25.62885, Accuracy = 91.60%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 67.54it/s]\n",
      "[4 / 30] Train: Loss = 13.07154, Accuracy = 92.98%: 100%|██████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.49it/s]\n",
      "[4 / 30]   Val: Loss = 21.73721, Accuracy = 93.00%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 68.60it/s]\n",
      "[5 / 30] Train: Loss = 10.09208, Accuracy = 94.33%: 100%|██████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.63it/s]\n",
      "[5 / 30]   Val: Loss = 17.16857, Accuracy = 93.84%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 70.46it/s]\n",
      "[6 / 30] Train: Loss = 8.13104, Accuracy = 95.23%: 100%|███████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.51it/s]\n",
      "[6 / 30]   Val: Loss = 18.08876, Accuracy = 94.42%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 67.01it/s]\n",
      "[7 / 30] Train: Loss = 6.62372, Accuracy = 95.92%: 100%|███████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.60it/s]\n",
      "[7 / 30]   Val: Loss = 16.78327, Accuracy = 94.73%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 66.67it/s]\n",
      "[8 / 30] Train: Loss = 5.51143, Accuracy = 96.43%: 100%|███████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.49it/s]\n",
      "[8 / 30]   Val: Loss = 17.98099, Accuracy = 94.96%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 68.42it/s]\n",
      "[9 / 30] Train: Loss = 4.68254, Accuracy = 96.83%: 100%|███████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.66it/s]\n",
      "[9 / 30]   Val: Loss = 15.51984, Accuracy = 95.20%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 68.97it/s]\n",
      "[10 / 30] Train: Loss = 3.99722, Accuracy = 97.16%: 100%|██████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.54it/s]\n",
      "[10 / 30]   Val: Loss = 17.00627, Accuracy = 95.27%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 67.55it/s]\n",
      "[11 / 30] Train: Loss = 3.53086, Accuracy = 97.42%: 100%|██████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.56it/s]\n",
      "[11 / 30]   Val: Loss = 17.80235, Accuracy = 95.30%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 68.78it/s]\n",
      "[12 / 30] Train: Loss = 3.10708, Accuracy = 97.66%: 100%|██████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.27it/s]\n",
      "[12 / 30]   Val: Loss = 18.60974, Accuracy = 95.36%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 68.97it/s]\n",
      "[13 / 30] Train: Loss = 2.74522, Accuracy = 97.84%: 100%|██████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.36it/s]\n",
      "[13 / 30]   Val: Loss = 17.13807, Accuracy = 95.44%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 70.08it/s]\n",
      "[14 / 30] Train: Loss = 2.44918, Accuracy = 98.02%: 100%|██████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.44it/s]\n",
      "[14 / 30]   Val: Loss = 17.33189, Accuracy = 95.49%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 67.72it/s]\n",
      "[15 / 30] Train: Loss = 2.18997, Accuracy = 98.17%: 100%|██████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.39it/s]\n",
      "[15 / 30]   Val: Loss = 18.66913, Accuracy = 95.44%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 69.33it/s]\n",
      "[16 / 30] Train: Loss = 1.95985, Accuracy = 98.33%: 100%|██████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.64it/s]\n",
      "[16 / 30]   Val: Loss = 17.79667, Accuracy = 95.43%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 67.01it/s]\n",
      "[17 / 30] Train: Loss = 1.78927, Accuracy = 98.46%: 100%|██████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.55it/s]\n",
      "[17 / 30]   Val: Loss = 18.41549, Accuracy = 95.42%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 68.24it/s]\n",
      "[18 / 30] Train: Loss = 1.56963, Accuracy = 98.60%: 100%|██████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.46it/s]\n",
      "[18 / 30]   Val: Loss = 18.67484, Accuracy = 95.41%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 68.60it/s]\n",
      "[19 / 30] Train: Loss = 1.42077, Accuracy = 98.69%: 100%|██████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.60it/s]\n",
      "[19 / 30]   Val: Loss = 18.28431, Accuracy = 95.36%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 69.33it/s]\n",
      "[20 / 30] Train: Loss = 1.29644, Accuracy = 98.82%: 100%|██████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.48it/s]\n",
      "[20 / 30]   Val: Loss = 20.81297, Accuracy = 95.41%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 71.23it/s]\n",
      "[21 / 30] Train: Loss = 1.15206, Accuracy = 98.93%: 100%|██████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.34it/s]\n",
      "[21 / 30]   Val: Loss = 18.37230, Accuracy = 95.38%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 69.33it/s]\n",
      "[22 / 30] Train: Loss = 1.04504, Accuracy = 99.00%: 100%|██████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.28it/s]\n",
      "[22 / 30]   Val: Loss = 19.49957, Accuracy = 95.37%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 70.46it/s]\n",
      "[23 / 30] Train: Loss = 0.94312, Accuracy = 99.10%: 100%|██████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.65it/s]\n",
      "[23 / 30]   Val: Loss = 21.20673, Accuracy = 95.26%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 66.17it/s]\n",
      "[24 / 30] Train: Loss = 0.86424, Accuracy = 99.14%: 100%|██████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.67it/s]\n",
      "[24 / 30]   Val: Loss = 20.56445, Accuracy = 95.27%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 70.46it/s]\n",
      "[25 / 30] Train: Loss = 0.75762, Accuracy = 99.24%: 100%|██████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.79it/s]\n",
      "[25 / 30]   Val: Loss = 20.75550, Accuracy = 95.25%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 70.84it/s]\n",
      "[26 / 30] Train: Loss = 0.72320, Accuracy = 99.28%: 100%|██████████████████████████████████████████████████| 572/572 [00:13<00:00, 43.93it/s]\n",
      "[26 / 30]   Val: Loss = 23.05351, Accuracy = 95.26%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 72.01it/s]\n",
      "[27 / 30] Train: Loss = 0.63471, Accuracy = 99.35%: 100%|██████████████████████████████████████████████████| 572/572 [00:12<00:00, 44.02it/s]\n",
      "[27 / 30]   Val: Loss = 26.22048, Accuracy = 95.21%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 70.08it/s]\n",
      "[28 / 30] Train: Loss = 0.55742, Accuracy = 99.42%: 100%|██████████████████████████████████████████████████| 572/572 [00:12<00:00, 44.13it/s]\n",
      "[28 / 30]   Val: Loss = 24.42090, Accuracy = 95.20%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 69.33it/s]\n",
      "[29 / 30] Train: Loss = 0.51261, Accuracy = 99.46%: 100%|██████████████████████████████████████████████████| 572/572 [00:12<00:00, 44.13it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[29 / 30]   Val: Loss = 26.94019, Accuracy = 95.16%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 69.71it/s]\n",
      "[30 / 30] Train: Loss = 0.49591, Accuracy = 99.47%: 100%|██████████████████████████████████████████████████| 572/572 [00:12<00:00, 44.28it/s]\n",
      "[30 / 30]   Val: Loss = 27.41317, Accuracy = 95.13%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 70.46it/s]\n"
     ]
    }
   ],
   "source": [
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=word_emb_dim, padding_idx=0)\n",
    "        self.bi_lstm = nn.LSTM(input_size=word_emb_dim, hidden_size=lstm_hidden_dim, num_layers=lstm_layers_count, proj_size=tagset_size, bidirectional=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        output, (hn, cn) = self.bi_lstm(x)\n",
    "        return output\n",
    "\n",
    "model = BiLSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=30,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: Loss = 16.63586, Accuracy = 95.13%: 100%|███████████████████████████████████████████████████████████| 224/224 [00:01<00:00, 126.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9513132210321927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = do_epoch(model, criterion, (X_test, y_test), 64, None, 'Test:')\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXmYGD_ANhm"
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZpY_Q1xZ18h"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mikhail\\desktop\\repo\\dlcourse_ai\\venv\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYogOoKlgtcf"
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsCstxiO03oT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know 38736 out of 45441 word embeddings\n"
     ]
    }
   ],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.vocab:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcG7i-R8hbY3"
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxaRBpQd0pat"
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.Tensor(embeddings), padding_idx=0)\n",
    "        self.bi_lstm = nn.LSTM(input_size=embeddings.shape[1], hidden_size=lstm_hidden_dim, num_layers=lstm_layers_count, proj_size=tagset_size, bidirectional=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        output, (hn, cn) = self.bi_lstm(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBtI6BDE-Fc7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 74.27585, Accuracy = 62.95%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.42it/s]\n",
      "[1 / 50]   Val: Loss = 57.20553, Accuracy = 82.03%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 47.23it/s]\n",
      "[2 / 50] Train: Loss = 24.49400, Accuracy = 88.37%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.83it/s]\n",
      "[2 / 50]   Val: Loss = 29.72511, Accuracy = 91.08%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 49.25it/s]\n",
      "[3 / 50] Train: Loss = 15.37870, Accuracy = 92.83%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.81it/s]\n",
      "[3 / 50]   Val: Loss = 21.41750, Accuracy = 93.30%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 47.58it/s]\n",
      "[4 / 50] Train: Loss = 11.90882, Accuracy = 94.19%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.91it/s]\n",
      "[4 / 50]   Val: Loss = 18.06623, Accuracy = 94.13%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 49.00it/s]\n",
      "[5 / 50] Train: Loss = 10.00163, Accuracy = 94.93%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.95it/s]\n",
      "[5 / 50]   Val: Loss = 18.10392, Accuracy = 94.57%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 47.40it/s]\n",
      "[6 / 50] Train: Loss = 8.92155, Accuracy = 95.35%: 100%|███████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.47it/s]\n",
      "[6 / 50]   Val: Loss = 14.14458, Accuracy = 95.13%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 48.55it/s]\n",
      "[7 / 50] Train: Loss = 8.12007, Accuracy = 95.69%: 100%|███████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.48it/s]\n",
      "[7 / 50]   Val: Loss = 15.07405, Accuracy = 94.95%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 49.66it/s]\n",
      "[8 / 50] Train: Loss = 7.62767, Accuracy = 95.85%: 100%|███████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.37it/s]\n",
      "[8 / 50]   Val: Loss = 16.27414, Accuracy = 94.59%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 46.45it/s]\n",
      "[9 / 50] Train: Loss = 6.89032, Accuracy = 96.09%: 100%|███████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.15it/s]\n",
      "[9 / 50]   Val: Loss = 12.50092, Accuracy = 95.58%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 51.52it/s]\n",
      "[10 / 50] Train: Loss = 6.62933, Accuracy = 96.23%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.18it/s]\n",
      "[10 / 50]   Val: Loss = 14.47079, Accuracy = 95.09%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 47.40it/s]\n",
      "[11 / 50] Train: Loss = 6.36817, Accuracy = 96.33%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.24it/s]\n",
      "[11 / 50]   Val: Loss = 13.73149, Accuracy = 95.71%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 47.40it/s]\n",
      "[12 / 50] Train: Loss = 6.11143, Accuracy = 96.45%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.18it/s]\n",
      "[12 / 50]   Val: Loss = 12.72173, Accuracy = 95.64%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 45.58it/s]\n",
      "[13 / 50] Train: Loss = 6.00930, Accuracy = 96.49%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 35.96it/s]\n",
      "[13 / 50]   Val: Loss = 12.49544, Accuracy = 95.86%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 47.05it/s]\n",
      "[14 / 50] Train: Loss = 5.53570, Accuracy = 96.70%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.15it/s]\n",
      "[14 / 50]   Val: Loss = 12.26303, Accuracy = 95.81%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 48.28it/s]\n",
      "[15 / 50] Train: Loss = 5.55120, Accuracy = 96.69%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.12it/s]\n",
      "[15 / 50]   Val: Loss = 11.34768, Accuracy = 95.88%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 48.10it/s]\n",
      "[16 / 50] Train: Loss = 5.40769, Accuracy = 96.75%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.37it/s]\n",
      "[16 / 50]   Val: Loss = 12.51935, Accuracy = 96.09%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 46.55it/s]\n",
      "[17 / 50] Train: Loss = 5.19261, Accuracy = 96.86%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.00it/s]\n",
      "[17 / 50]   Val: Loss = 11.72363, Accuracy = 96.09%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 46.22it/s]\n",
      "[18 / 50] Train: Loss = 5.21302, Accuracy = 96.84%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.08it/s]\n",
      "[18 / 50]   Val: Loss = 11.30988, Accuracy = 95.91%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 48.82it/s]\n",
      "[19 / 50] Train: Loss = 5.02485, Accuracy = 96.92%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.03it/s]\n",
      "[19 / 50]   Val: Loss = 12.92294, Accuracy = 96.09%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 46.64it/s]\n",
      "[20 / 50] Train: Loss = 4.98369, Accuracy = 96.95%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.12it/s]\n",
      "[20 / 50]   Val: Loss = 11.61470, Accuracy = 96.19%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 48.31it/s]\n",
      "[21 / 50] Train: Loss = 4.84547, Accuracy = 96.99%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.25it/s]\n",
      "[21 / 50]   Val: Loss = 13.65768, Accuracy = 95.68%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 46.89it/s]\n",
      "[22 / 50] Train: Loss = 4.72307, Accuracy = 97.05%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 35.95it/s]\n",
      "[22 / 50]   Val: Loss = 11.42906, Accuracy = 96.19%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 48.10it/s]\n",
      "[23 / 50] Train: Loss = 4.65512, Accuracy = 97.07%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.11it/s]\n",
      "[23 / 50]   Val: Loss = 11.32438, Accuracy = 96.01%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 48.46it/s]\n",
      "[24 / 50] Train: Loss = 4.68501, Accuracy = 97.04%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.03it/s]\n",
      "[24 / 50]   Val: Loss = 12.00714, Accuracy = 96.29%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 49.75it/s]\n",
      "[25 / 50] Train: Loss = 4.61478, Accuracy = 97.08%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.41it/s]\n",
      "[25 / 50]   Val: Loss = 11.67543, Accuracy = 96.45%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 46.72it/s]\n",
      "[26 / 50] Train: Loss = 4.40415, Accuracy = 97.17%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.62it/s]\n",
      "[26 / 50]   Val: Loss = 12.15266, Accuracy = 96.06%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 48.27it/s]\n",
      "[27 / 50] Train: Loss = 4.60434, Accuracy = 97.09%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.93it/s]\n",
      "[27 / 50]   Val: Loss = 12.34709, Accuracy = 95.98%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 47.75it/s]\n",
      "[28 / 50] Train: Loss = 4.38491, Accuracy = 97.18%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.25it/s]\n",
      "[28 / 50]   Val: Loss = 10.24358, Accuracy = 96.35%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 48.64it/s]\n",
      "[29 / 50] Train: Loss = 4.31992, Accuracy = 97.21%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.12it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[29 / 50]   Val: Loss = 10.93441, Accuracy = 96.12%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 49.00it/s]\n",
      "[30 / 50] Train: Loss = 4.21836, Accuracy = 97.26%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.19it/s]\n",
      "[30 / 50]   Val: Loss = 11.24934, Accuracy = 96.40%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 47.57it/s]\n",
      "[31 / 50] Train: Loss = 4.56608, Accuracy = 97.11%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.22it/s]\n",
      "[31 / 50]   Val: Loss = 10.51375, Accuracy = 96.26%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 50.32it/s]\n",
      "[32 / 50] Train: Loss = 4.17179, Accuracy = 97.28%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.21it/s]\n",
      "[32 / 50]   Val: Loss = 11.53984, Accuracy = 96.34%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 47.82it/s]\n",
      "[33 / 50] Train: Loss = 4.24135, Accuracy = 97.24%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.21it/s]\n",
      "[33 / 50]   Val: Loss = 12.01196, Accuracy = 96.37%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 46.89it/s]\n",
      "[34 / 50] Train: Loss = 4.18765, Accuracy = 97.26%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.30it/s]\n",
      "[34 / 50]   Val: Loss = 11.75663, Accuracy = 96.31%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 47.92it/s]\n",
      "[35 / 50] Train: Loss = 4.16310, Accuracy = 97.29%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.41it/s]\n",
      "[35 / 50]   Val: Loss = 9.63997, Accuracy = 96.35%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 48.73it/s]\n",
      "[36 / 50] Train: Loss = 4.07900, Accuracy = 97.31%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.28it/s]\n",
      "[36 / 50]   Val: Loss = 10.52695, Accuracy = 96.30%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 47.40it/s]\n",
      "[37 / 50] Train: Loss = 3.99956, Accuracy = 97.36%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.32it/s]\n",
      "[37 / 50]   Val: Loss = 11.09711, Accuracy = 96.24%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 48.82it/s]\n",
      "[38 / 50] Train: Loss = 4.10162, Accuracy = 97.32%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.14it/s]\n",
      "[38 / 50]   Val: Loss = 11.32011, Accuracy = 96.39%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 47.92it/s]\n",
      "[39 / 50] Train: Loss = 4.06769, Accuracy = 97.32%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.34it/s]\n",
      "[39 / 50]   Val: Loss = 10.48060, Accuracy = 96.44%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 47.57it/s]\n",
      "[40 / 50] Train: Loss = 3.99240, Accuracy = 97.35%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.34it/s]\n",
      "[40 / 50]   Val: Loss = 10.03445, Accuracy = 96.42%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 50.72it/s]\n",
      "[41 / 50] Train: Loss = 4.00436, Accuracy = 97.37%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.25it/s]\n",
      "[41 / 50]   Val: Loss = 10.27190, Accuracy = 96.48%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 49.00it/s]\n",
      "[42 / 50] Train: Loss = 3.92846, Accuracy = 97.38%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.38it/s]\n",
      "[42 / 50]   Val: Loss = 10.83889, Accuracy = 96.56%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 46.55it/s]\n",
      "[43 / 50] Train: Loss = 4.04374, Accuracy = 97.34%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.12it/s]\n",
      "[43 / 50]   Val: Loss = 10.96815, Accuracy = 96.52%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 48.73it/s]\n",
      "[44 / 50] Train: Loss = 3.96374, Accuracy = 97.38%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.24it/s]\n",
      "[44 / 50]   Val: Loss = 10.09552, Accuracy = 96.58%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 47.92it/s]\n",
      "[45 / 50] Train: Loss = 3.78430, Accuracy = 97.45%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.46it/s]\n",
      "[45 / 50]   Val: Loss = 9.64351, Accuracy = 96.55%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 48.37it/s]\n",
      "[46 / 50] Train: Loss = 3.89104, Accuracy = 97.42%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.26it/s]\n",
      "[46 / 50]   Val: Loss = 9.59512, Accuracy = 96.51%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 48.64it/s]\n",
      "[47 / 50] Train: Loss = 3.75660, Accuracy = 97.47%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.43it/s]\n",
      "[47 / 50]   Val: Loss = 10.49964, Accuracy = 96.55%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 48.28it/s]\n",
      "[48 / 50] Train: Loss = 3.74296, Accuracy = 97.50%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.31it/s]\n",
      "[48 / 50]   Val: Loss = 10.97499, Accuracy = 96.38%: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 47.75it/s]\n",
      "[49 / 50] Train: Loss = 3.85801, Accuracy = 97.39%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.31it/s]\n",
      "[49 / 50]   Val: Loss = 9.24796, Accuracy = 96.66%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 49.94it/s]\n",
      "[50 / 50] Train: Loss = 3.86929, Accuracy = 97.41%: 100%|██████████████████████████████████████████████████| 572/572 [00:15<00:00, 36.19it/s]\n",
      "[50 / 50]   Val: Loss = 9.68520, Accuracy = 96.70%: 100%|████████████████████████████████████████████████████| 13/13 [00:00<00:00, 47.57it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=embeddings,\n",
    "    tagset_size=len(tag2ind),\n",
    "    lstm_layers_count=2\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), weight_decay=1e-2)\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ne_8f24h8kg"
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
    "\n",
    "Добейтесь качества лучше прошлых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPUuAPGhEGVR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: Loss = 7.19588, Accuracy = 96.70%: 100%|█████████████████████████████████████████████████████████████| 224/224 [00:02<00:00, 99.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9670360692535176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = do_epoch(model, criterion, (X_test, y_test), 64, None, 'Test:')\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 06 - RNNs, part 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
